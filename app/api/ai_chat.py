"""
AI Chat API â€” Unified AI-powered store building via chat.

Multi-provider AI chain:
  1. Anthropic Claude (primary)
  2. OpenAI GPT (fallback)
  3. Google Gemini (second fallback)
  4. Local modifications (offline fallback)

With Supabase integration for conversation storage.
"""

from typing import Annotated, Optional
import time
import logging

import httpx

from fastapi import APIRouter, Depends, Request, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import get_settings
from app.database import get_db
from app.middleware.auth import CurrentUser
from app.middleware.rate_limit import limiter
from app.schemas.ai_chat import (
    AIChatRequest,
    AIChatResponse,
    AIConversationRequest,
    AIConversationResponse,
)

router = APIRouter()
settings = get_settings()
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Supabase Helper for Saving Conversations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _save_conversation_to_supabase(
    user_id: str,
    store_id: Optional[str],
    message: str,
    response: str,
    html_before: Optional[str] = None,
    html_after: Optional[str] = None,
    execution_time: Optional[float] = None,
):
    """Save AI conversation to Supabase for learning and analytics."""
    if not settings.SUPABASE_URL or not settings.SUPABASE_SERVICE_KEY:
        return  # Skip if Supabase not configured
    
    try:
        headers = {
            "apikey": settings.SUPABASE_SERVICE_KEY,
            "Authorization": f"Bearer {settings.SUPABASE_SERVICE_KEY}",
            "Content-Type": "application/json",
            "Prefer": "return=minimal",
        }
        
        data = {
            "user_id": user_id,
            "messages": [
                {"role": "user", "content": message},
                {"role": "assistant", "content": response},
            ],
        }
        
        if store_id:
            data["project_id"] = store_id
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            await client.post(
                f"{settings.SUPABASE_URL}/rest/v1/chat_history",
                headers=headers,
                json=data,
            )
    except Exception as e:
        # Non-blocking - just log the error
        print(f"âš ï¸ Supabase save error (non-critical): {e}")


CONVERSATION_SYSTEM_PROMPT = """Ø£Ù†Øª Ù…Ø·ÙˆØ± ÙˆÙŠØ¨ Ù…Ø­ØªØ±Ù ÙˆÙ…Ø³ØªØ´Ø§Ø± Ø¨Ù†Ø§Ø¡ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø±Ù‚Ù…ÙŠØ© Ø¨Ù…Ø³ØªÙˆÙ‰ v0.dev â€” Ø§Ø³Ù…Ùƒ "WebFlow AI".

## Ø´Ø®ØµÙŠØªÙƒ
- Ø®Ø¨ÙŠØ± ØªØµÙ…ÙŠÙ… ÙˆØªØ·ÙˆÙŠØ± ÙˆÙŠØ¨ Ø¨Ø®Ø¨Ø±Ø© 15+ Ø³Ù†Ø©
- ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø­ØªØ±ÙØŒ ØªØªÙƒÙ„Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø³Ù‡Ù„ ÙˆÙ…ÙÙ‡ÙˆÙ…
- ØªÙÙ‡Ù… Ù…Ø§ ÙŠØ±ÙŠØ¯Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ÙŠØ¹Ø¨Ø± Ø¨ÙˆØ¶ÙˆØ­
- ØªÙ‚ØªØ±Ø­ Ø£ÙÙƒØ§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª Ø°ÙƒÙŠØ©
- Ø±Ø¯ÙˆØ¯Ùƒ Ù…Ø±ÙƒØ²Ø© ÙˆÙ…ÙÙŠØ¯Ø© (3-5 Ø¬Ù…Ù„)

## Ù…Ù‡Ù…ØªÙƒ
1. ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø±Ø¤ÙŠØªÙ‡ Ù„Ù…Ø´Ø±ÙˆØ¹Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Ù…ØªØ¬Ø±ØŒ Ù…ÙˆÙ‚Ø¹ØŒ Ù…Ø­ÙØ¸Ø© Ø£Ø¹Ù…Ø§Ù„ØŒ Ù…Ø¯ÙˆÙ†Ø©ØŒ Ù…Ø·Ø¹Ù…ØŒ Ø¥Ù„Ø®)
2. ØªØ³Ø£Ù„Ù‡ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© Ø¹Ù†: Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø§Ù„Ø³ØªØ§ÙŠÙ„ØŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ØŒ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
3. ØªÙ‚ØªØ±Ø­ Ø¹Ù„ÙŠÙ‡ Ø£Ù‚Ø³Ø§Ù… ÙˆÙ…Ù…ÙŠØ²Ø§Øª ØªÙ†Ø§Ø³Ø¨ Ù…Ø´Ø±ÙˆØ¹Ù‡
4. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²ØŒ ØªÙˆØ¬Ù‡Ù‡ Ù„Ù‚ÙˆÙ„ "Ù†ÙÙ‘Ø°" Ø£Ùˆ "Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ù†Ø§Ø¡"

## Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
- **Ù…ØªØ§Ø¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©**: Ø£Ø²ÙŠØ§Ø¡ØŒ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§ØªØŒ Ø¹Ø·ÙˆØ±ØŒ Ø£ØºØ°ÙŠØ©ØŒ Ù…Ø¬ÙˆÙ‡Ø±Ø§ØªØŒ Ø±ÙŠØ§Ø¶Ø©ØŒ Ø£Ø·ÙØ§Ù„
- **Ù…ÙˆØ§Ù‚Ø¹ Ø®Ø¯Ù…ÙŠØ©**: Ù…Ø·Ø§Ø¹Ù…ØŒ Ø´Ø±ÙƒØ§ØªØŒ Ø¹ÙŠØ§Ø¯Ø§ØªØŒ Ù…ÙƒØ§ØªØ¨ Ù…Ø­Ø§Ù…Ø§Ø©ØŒ Ù…ÙƒØ§ØªØ¨ Ø¹Ù‚Ø§Ø±ÙŠØ©
- **Ù…ÙˆØ§Ù‚Ø¹ Ø´Ø®ØµÙŠØ©**: Ù…Ø­ÙØ¸Ø© Ø£Ø¹Ù…Ø§Ù„ (Portfolio)ØŒ Ø³ÙŠØ±Ø© Ø°Ø§ØªÙŠØ©ØŒ Ù…Ø¯ÙˆÙ†Ø© Ø´Ø®ØµÙŠØ©
- **Ù…ÙˆØ§Ù‚Ø¹ ØªØ¹Ù„ÙŠÙ…ÙŠØ©**: Ù…Ù†ØµØ© ÙƒÙˆØ±Ø³Ø§ØªØŒ Ù…Ø¯Ø±Ø³Ø©ØŒ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©
- **ØµÙØ­Ø§Øª Ù‡Ø¨ÙˆØ·**: Landing pages Ù„Ù…Ù†ØªØ¬Ø§Øª Ø£Ùˆ Ø­Ù…Ù„Ø§Øª ØªØ³ÙˆÙŠÙ‚ÙŠØ©

## Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©
- Ù„Ø§ ØªÙˆÙ„Ù‘Ø¯ Ø£ÙŠ ÙƒÙˆØ¯ â€” ÙÙ‚Ø· Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ø³ØªØ´Ø§Ø±ÙŠØ©
- Ø§Ù‚ØªØ±Ø­ Ø£Ù‚Ø³Ø§Ù… Ù…Ù†Ø§Ø³Ø¨Ø©: HeroØŒ Ø®Ø¯Ù…Ø§ØªØŒ Ù…Ù†ØªØ¬Ø§ØªØŒ Ø¹Ø±ÙˆØ¶ØŒ ØªÙ‚ÙŠÙŠÙ…Ø§ØªØŒ Ø¹Ù†Ù‘Ø§ØŒ FAQØŒ ØªÙˆØ§ØµÙ„ØŒ ÙÙˆØªØ±
- ÙƒÙ† Ù…Ø¨Ø¯Ø¹Ø§Ù‹ ÙÙŠ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª â€” ÙÙƒØ± Ø¨Ù…Ø§ ÙŠÙ…ÙŠØ² Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¹Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
- Ø¥Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„ "Ù†ÙÙ‘Ø°" Ø£Ùˆ "Ø§Ø¨Ø¯Ø£" ÙˆØ¬Ù‘Ù‡Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©

## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
- Ø§Ø³Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {store_name}
- Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {store_type}"""


CHAT_SYSTEM_PROMPT = """Ø£Ù†Øª WebFlow AI Pro â€” Ù…Ø­Ø±Ùƒ Ø¨Ù†Ø§Ø¡ Ù…ÙˆØ§Ù‚Ø¹ ÙˆÙŠØ¨ Ø¨Ù…Ø³ØªÙˆÙ‰ v0.dev ÙˆÙ…Ø¹ÙŠØ§Ø± Vercel.

## Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
ØªØ­ÙˆÙŠÙ„ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ ÙƒÙˆØ¯ HTML/CSS/JS ÙƒØ§Ù…Ù„ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠ Ù„Ù…ÙˆØ§Ù‚Ø¹ ÙˆÙ…ØªØ§Ø¬Ø± Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙ‰.

## Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµØ§Ø±Ù…Ø©
1. Ø£Ø±Ø¬Ø¹ HTML ÙƒØ§Ù…Ù„ ÙÙ‚Ø· â€” Ù…Ù† <!DOCTYPE html> Ø¥Ù„Ù‰ </html>
2. Ù„Ø§ ØªØ¶Ù Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ markdown â€” ÙÙ‚Ø· ÙƒÙˆØ¯ HTML Ù†Ø¸ÙŠÙ
3. Ø¶Ø¹ ÙƒÙ„ CSS Ø¯Ø§Ø®Ù„ <style> ÙÙŠ <head>
4. Ø¶Ø¹ ÙƒÙ„ JavaScript Ø¯Ø§Ø®Ù„ <script> Ù‚Ø¨Ù„ </body>
5. Ø§Ø³ØªØ®Ø¯Ù… font-family: 'Cairo', sans-serif Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
6. Ø£Ø¶Ù Google Fonts link Ù„Ù„Ø®Ø· Cairo ÙÙŠ <head>
7. ÙƒÙ„ Ø§Ù„ØªØµÙ…ÙŠÙ… RTL (dir="rtl" lang="ar")
8. Ø§Ù„ØªØµÙ…ÙŠÙ… responsive Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (mobile-first)

## Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØµÙ…ÙŠÙ… (v0-level)
### Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª
- Ø§Ø³ØªØ®Ø¯Ù… CSS Variables: --primary, --primary-dark, --secondary, --accent, --bg, --surface, --text, --text-muted
- ØªØ¯Ø±Ø¬Ø§Øª Ø§Ø­ØªØ±Ø§ÙÙŠØ©: linear-gradient(135deg, var(--primary), var(--accent))
- Ø§Ø³ØªØ®Ø¯Ù… backdrop-filter: blur() Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø²Ø¬Ø§Ø¬ÙŠØ©
- Ø¸Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª: box-shadow: 0 4px 6px -1px rgba(0,0,0,.1), 0 2px 4px -2px rgba(0,0,0,.1)

### Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„ØªÙØ§Ø¹Ù„
- transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1) Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
- hover effects: scale(1.02), translateY(-4px), shadow uplift
- Ø£Ø¶Ù CSS @keyframes Ù„Ù„Ø­Ø±ÙƒØ§Øª: fadeInUp, slideIn, pulse, float
- Ø£Ø¶Ù Intersection Observer JS Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¸Ù‡ÙˆØ± Ø¹Ù†Ø¯ Ø§Ù„ØªÙ…Ø±ÙŠØ±

### Ø§Ù„ØªØ®Ø·ÙŠØ·
- CSS Grid + Flexbox (Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… float Ø£Ø¨Ø¯Ø§Ù‹)
- max-width: 1200px Ù„Ù„Ø­Ø§ÙˆÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ padding: 0 24px
- spacing Ù…ØªÙ†Ø§Ø³Ù‚ Ø¹Ù„Ù‰ grid 8px
- border-radius: 16px Ù„Ù„Ø¨Ø·Ø§Ù‚Ø§Øª, 12px Ù„Ù„Ø£Ø²Ø±Ø§Ø±, 9999px Ù„Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©

### Ø§Ù„Ø·Ø¨Ø§Ø¹Ø©
- Ø¹Ù†Ø§ÙˆÙŠÙ†: font-weight: 800-900, line-height: 1.2
- Ù†ØµÙˆØµ: font-weight: 400, line-height: 1.7
- ØªØ¯Ø±Ø¬ Ø­Ø¬Ù…: clamp() Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ù…Ø«Ø§Ù„: clamp(1.5rem, 4vw, 3rem))

## Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø©
- **Hero**: Ø¹Ù†ÙˆØ§Ù† Ø¶Ø®Ù… + ÙˆØµÙ + CTA + gradient/ØµÙˆØ±Ø© Ø®Ù„ÙÙŠØ© + Ø­Ø±ÙƒØ§Øª
- **Ø´Ø±ÙŠØ· Ù…Ø²Ø§ÙŠØ§**: Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª + Ù†ØµÙˆØµ (ØªÙˆØµÙŠÙ„ Ù…Ø¬Ø§Ù†ÙŠØŒ Ø¯ÙØ¹ Ø¢Ù…Ù†ØŒ Ø¥Ø±Ø¬Ø§Ø¹ Ø³Ù‡Ù„)
- **Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª/Ø§Ù„Ø®Ø¯Ù…Ø§Øª**: Grid Ø¨Ø·Ø§Ù‚Ø§Øª Ù…Ø¹ ØµÙˆØ±ØŒ Ø£Ø³Ø¹Ø§Ø±ØŒ Ø£Ø²Ø±Ø§Ø±ØŒ badges
- **Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª**: Grid Ø¯Ø§Ø¦Ø±ÙŠ Ø£Ùˆ Ø¨Ø·Ø§Ù‚Ø§Øª Ù…Ø¹ ØµÙˆØ± ÙˆØ¹Ù†Ø§ÙˆÙŠÙ†
- **Ø§Ù„Ø¹Ø±ÙˆØ¶/Ø§Ù„ØªØ®ÙÙŠØ¶Ø§Øª**: Banner Ù…Ù„ÙˆÙ† + Ø¹Ø¯Ø§Ø¯ ØªÙ†Ø§Ø²Ù„ÙŠ + CTA
- **Ø¹Ù†Ù‘Ø§**: Ù†Øµ + ØµÙˆØ±Ø© + Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª (Ø£Ø±Ù‚Ø§Ù… Ù…ØªØ­Ø±ÙƒØ©)
- **Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª**: Ø¨Ø·Ø§Ù‚Ø§Øª Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø¹ Ù†Ø¬ÙˆÙ… ÙˆØµÙˆØ±
- **Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©**: Accordion Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø·ÙŠ
- **Ø§Ù„Ù†Ø´Ø±Ø© Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ©**: Input + Button + Ù†Øµ ØªØ­ÙÙŠØ²ÙŠ
- **Ø§Ù„ØªÙˆØ§ØµÙ„**: Ù†Ù…ÙˆØ°Ø¬ + Ø®Ø±ÙŠØ·Ø© + Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„
- **Ø§Ù„ÙÙˆØªØ±**: Ø£Ø¹Ù…Ø¯Ø© Ø±ÙˆØ§Ø¨Ø· + Ø´Ø¹Ø§Ø± + ØªÙˆØ§ØµÙ„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ + Ø­Ù‚ÙˆÙ‚

## Ø¹Ù†Ø¯ ØªØ¹Ø¯ÙŠÙ„ ÙƒÙˆØ¯ Ù…ÙˆØ¬ÙˆØ¯
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ø§ Ù„Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØºÙŠÙŠØ±Ù‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
- Ø·Ø¨Ù‘Ù‚ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙÙ‚Ø· Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
- Ù„Ø§ ØªØ­Ø°Ù Ø£Ù‚Ø³Ø§Ù… Ù„Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø°ÙÙ‡Ø§
- Ø­Ø³Ù‘Ù† Ø§Ù„ÙƒÙˆØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¶Ø¹ÙŠÙØ§Ù‹ â€” Ø£Ù†Øª Pro

## Ù…Ù„Ø§Ø­Ø¸Ø§Øª
- Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¨Ø§Ù„Ø±ÙŠØ§Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ (Ø±.Ø³) Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… emoji Ø¨Ø°ÙƒØ§Ø¡ ÙƒØ£ÙŠÙ‚ÙˆÙ†Ø§Øª: ğŸ›ï¸ ğŸšš ğŸ’ â­ ğŸ”¥
- Ø§Ù„Ø£Ø²Ø±Ø§Ø± ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¹Ø¨Ø±Ø© Ù…Ø¹ hover states
- ÙƒÙ„ Ù‚Ø³Ù… ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ â€” Ù„Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ JS Ø®Ø§Ø±Ø¬ÙŠ"""


async def _call_anthropic_chat(current_html: str, user_message: str, api_key: str) -> str:
    """Call Anthropic Claude to modify the store HTML."""
    import anthropic

    client = anthropic.AsyncAnthropic(api_key=api_key)
    message = await client.messages.create(
        model=settings.CLAUDE_MODEL,
        max_tokens=settings.AI_MAX_TOKENS,
        system=CHAT_SYSTEM_PROMPT,
        messages=[
            {
                "role": "user",
                "content": f"Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ:\n```html\n{current_html}\n```\n\nØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_message}\n\nØ£Ø±Ø¬Ø¹ HTML Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¹Ø¯Ù‘Ù„ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ markdown):",
            },
        ],
    )
    content = message.content[0].text
    return _clean_ai_response(content)


async def _call_openai_chat(current_html: str, user_message: str, api_key: str) -> str:
    """Call OpenAI GPT to modify the store HTML (first fallback)."""
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key=api_key, timeout=90.0)
    response = await client.chat.completions.create(
        model=settings.GPT_MODEL,
        messages=[
            {"role": "system", "content": CHAT_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": f"Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ:\n```html\n{current_html}\n```\n\nØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_message}\n\nØ£Ø±Ø¬Ø¹ HTML Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¹Ø¯Ù‘Ù„ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ markdown):",
            },
        ],
        temperature=settings.AI_TEMPERATURE,
        max_tokens=settings.AI_MAX_TOKENS,
    )
    content = response.choices[0].message.content or ""
    return _clean_ai_response(content)


async def _call_gemini_chat(current_html: str, user_message: str, api_key: str) -> str:
    """Call Google Gemini to modify the store HTML (second fallback)."""
    import google.generativeai as genai

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(
        model_name=settings.GEMINI_MODEL,
        system_instruction=CHAT_SYSTEM_PROMPT,
    )

    prompt = f"Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ:\n```html\n{current_html}\n```\n\nØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_message}\n\nØ£Ø±Ø¬Ø¹ HTML Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¹Ø¯Ù‘Ù„ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ markdown):"
    
    response = await model.generate_content_async(
        prompt,
        generation_config=genai.GenerationConfig(
            temperature=settings.AI_TEMPERATURE,
            max_output_tokens=settings.AI_MAX_TOKENS,
        ),
    )
    content = response.text or ""
    return _clean_ai_response(content)


def _clean_ai_response(content: str) -> str:
    """Remove markdown code fences from AI responses."""
    content = content.strip()
    if content.startswith("```"):
        lines = content.split("\n")
        # Remove first line (```html or ```)
        lines = lines[1:]
        # Remove last line if it's ```
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        content = "\n".join(lines)
    return content.strip()


def _apply_local_modifications(current_html: str, message: str) -> tuple[str, str]:
    """Apply basic color/text modifications locally when no API key is available."""
    html = current_html
    changes: list[str] = []

    color_map = {
        "Ø£Ø®Ø¶Ø±": ("#00b894", "#00a085"),
        "Ø£Ø­Ù…Ø±": ("#e74c3c", "#c0392b"),
        "Ø£Ø²Ø±Ù‚": ("#0984e3", "#0652DD"),
        "Ø°Ù‡Ø¨ÙŠ": ("#d4af37", "#b8960c"),
        "Ø¨Ø±ØªÙ‚Ø§Ù„ÙŠ": ("#e17055", "#d63031"),
        "ÙˆØ±Ø¯ÙŠ": ("#fd79a8", "#e84393"),
        "Ø¨Ù†ÙØ³Ø¬ÙŠ": ("#6c5ce7", "#4834d4"),
        "Ø£Ø³ÙˆØ¯": ("#2d3436", "#1e272e"),
        "ÙƒØ­Ù„ÙŠ": ("#2c3e50", "#1a252f"),
    }

    for color_name, (primary, dark) in color_map.items():
        if color_name in message:
            html = html.replace("#6c5ce7", primary).replace("#4834d4", dark)
            changes.append(f"ØªÙ… ØªØºÙŠÙŠØ± Ø§Ù„Ù„ÙˆÙ† Ø¥Ù„Ù‰ {color_name}")
            break

    if "ÙØ§Ø®Ø±" in message or "luxury" in message.lower():
        html = html.replace("background: #fafafa", "background: #0a0a1a")
        html = html.replace("color: #1a1a2e", "color: #f0e6d2")
        html = html.replace("background: white", "background: #1a1a2e")
        html = html.replace("background: #f8f8fc", "background: #0d0d20")
        html = html.replace("#6c5ce7", "#d4af37").replace("#4834d4", "#1a0a2e")
        changes.append("ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØµÙ…ÙŠÙ… Ù„Ø³ØªØ§ÙŠÙ„ ÙØ§Ø®Ø±")

    if "Ø¯Ø§ÙƒÙ†" in message or "dark" in message.lower():
        html = html.replace("background: #fafafa", "background: #0f0f23")
        html = html.replace("color: #1a1a2e", "color: #e0e0e0")
        html = html.replace("background: white", "background: #1a1a3e")
        html = html.replace("background: #f8f8fc", "background: #16163a")
        html = html.replace("color: #444", "color: #ccc")
        html = html.replace("color: #666", "color: #999")
        html = html.replace("border-bottom: 1px solid #eee", "border-bottom: 1px solid #333")
        changes.append("ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¯Ø§ÙƒÙ†")

    if "6 Ù…Ù†ØªØ¬Ø§Øª" in message or "Ù…Ù†ØªØ¬Ø§Øª Ø£ÙƒØ«Ø±" in message or "Ø£Ø¶Ù Ù…Ù†ØªØ¬Ø§Øª" in message:
        extra_products = """
      <div class="product-card"><div class="product-img">ğŸ</div><div class="info"><div class="name">Ù…Ù†ØªØ¬ Ø­ØµØ±ÙŠ 5</div><div class="price">299 Ø±.Ø³</div></div></div>
      <div class="product-card"><div class="product-img">ğŸ›ï¸</div><div class="info"><div class="name">Ù…Ù†ØªØ¬ Ù…Ù…ÙŠØ² 6</div><div class="price">349 Ø±.Ø³</div></div></div>"""
        html = html.replace(
            '</div>\n  </div>\n  <div class="features">',
            f'{extra_products}\n    </div>\n  </div>\n  <div class="features">',
        )
        changes.append("ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…Ù†ØªØ¬Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©")

    if "Ø¹Ø±ÙˆØ¶" in message or "ØªØ®ÙÙŠØ¶Ø§Øª" in message:
        offers_section = """
  <div style="background: linear-gradient(135deg, #e74c3c, #c0392b); padding: 40px 24px; text-align: center; color: white;">
    <h2 style="font-size: 1.8rem; font-weight: 800; margin-bottom: 8px;">ğŸ”¥ Ø¹Ø±ÙˆØ¶ Ø­ØµØ±ÙŠØ©</h2>
    <p style="font-size: 1.1rem; opacity: 0.9; margin-bottom: 16px;">Ø®ØµÙˆÙ…Ø§Øª ØªØµÙ„ Ø¥Ù„Ù‰ 50% Ø¹Ù„Ù‰ Ù…Ù†ØªØ¬Ø§Øª Ù…Ø®ØªØ§Ø±Ø©</p>
    <button style="background: white; color: #e74c3c; border: none; padding: 12px 28px; border-radius: 10px; font-weight: 700; font-size: 1rem; cursor: pointer; font-family: 'Tajawal', sans-serif;">ØªØ³ÙˆÙ‚ Ø§Ù„Ø¹Ø±ÙˆØ¶</button>
  </div>"""
        html = html.replace(
            '<div class="features">', f'{offers_section}\n  <div class="features">'
        )
        changes.append("ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‚Ø³Ù… Ø§Ù„Ø¹Ø±ÙˆØ¶")

    if "Ø¨Ø§Ù†Ø±" in message:
        html = html.replace(
            "padding: 80px 24px",
            "padding: 100px 24px; background-size: cover; background-position: center",
        )
        html = html.replace(
            "font-size: 2.5rem", "font-size: 3rem; text-shadow: 2px 2px 8px rgba(0,0,0,0.3)"
        )
        changes.append("ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø§Ù†Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ")

    if not changes:
        changes.append("ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª")

    return html, " â€” ".join(changes)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Conversation AI (chat without HTML generation)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _call_anthropic_conversation(
    messages: list[dict],
    system_prompt: str,
    api_key: str,
) -> str:
    """Call Anthropic Claude for conversation (no HTML)."""
    import anthropic

    client = anthropic.AsyncAnthropic(api_key=api_key)
    response = await client.messages.create(
        model=settings.CLAUDE_MODEL,
        max_tokens=1024,
        system=system_prompt,
        messages=messages,
    )
    return response.content[0].text


async def _call_openai_conversation(
    messages: list[dict],
    system_prompt: str,
    api_key: str,
) -> str:
    """Call OpenAI for conversation (no HTML)."""
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key=api_key, timeout=30.0)
    all_messages = [{"role": "system", "content": system_prompt}] + messages
    response = await client.chat.completions.create(
        model=settings.GPT_MODEL,
        messages=all_messages,
        temperature=0.8,
        max_tokens=1024,
    )
    return response.choices[0].message.content or ""


async def _call_gemini_conversation(
    messages: list[dict],
    system_prompt: str,
    api_key: str,
) -> str:
    """Call Gemini for conversation (no HTML)."""
    import google.generativeai as genai

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(
        model_name=settings.GEMINI_MODEL,
        system_instruction=system_prompt,
    )

    # Convert messages to Gemini format
    history_text = "\n".join(
        f"{'Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' if m['role'] == 'user' else 'Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯'}: {m['content']}"
        for m in messages[:-1]
    )
    last_msg = messages[-1]["content"] if messages else ""
    prompt = f"{history_text}\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {last_msg}" if history_text else last_msg

    response = await model.generate_content_async(
        prompt,
        generation_config=genai.GenerationConfig(temperature=0.8, max_output_tokens=1024),
    )
    return response.text or ""


def _get_conversation_suggestions(message: str, store_type: str) -> list[str]:
    """Return conversation suggestions based on context."""
    suggestions_map = {
        "fashion": [
            "Ø£Ø¨ÙŠ Ø£Ù„ÙˆØ§Ù† Ø£Ù†Ø«ÙˆÙŠØ© Ù…Ø«Ù„ Ø§Ù„ÙˆØ±Ø¯ÙŠ ÙˆØ§Ù„Ø°Ù‡Ø¨ÙŠ",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ø£Ø­Ø¯Ø« Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù…Ø¹ ØµÙˆØ±",
            "Ø£Ø¨ÙŠ Ø¹Ø±ÙˆØ¶ Ù…ÙˆØ³Ù…ÙŠØ© Ù…Ø¹ Ø®ØµÙˆÙ…Ø§Øª",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "electronics": [
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… ØªÙ‚Ù†ÙŠ Ø­Ø¯ÙŠØ« Ø¨Ø£Ù„ÙˆØ§Ù† Ø²Ø±Ù‚Ø§Ø¡ Ø¯Ø§ÙƒÙ†Ø©",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ø¨ÙŠØ¹Ø§Ù‹",
            "Ø£Ø¨ÙŠ Ù‚Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "food": [
            "Ø£Ø¨ÙŠ Ø£Ù„ÙˆØ§Ù† Ø¯Ø§ÙØ¦Ø© ØªØ´Ù‡Ù‘ÙŠ Ù…Ø¹ ØµÙˆØ± Ø£Ø·Ø¨Ø§Ù‚",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø·Ø¹Ø§Ù… Ù…ØµÙ†ÙØ©",
            "Ø£Ø¨ÙŠ Ù‚Ø³Ù… ØªÙˆØµÙŠÙ„ Ù…Ø¹ Ø£ÙˆÙ‚Ø§Øª ÙˆÙ…Ù†Ø§Ø·Ù‚",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "restaurant": [
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… Ø£Ù†ÙŠÙ‚ Ù…Ø¹ ØµÙˆØ± Ø£Ø·Ø¨Ø§Ù‚ ÙƒØ¨ÙŠØ±Ø©",
            "Ø£Ø¶Ù Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø·Ø¹Ø§Ù… Ù…Ø¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±",
            "Ø£Ø¨ÙŠ Ù‚Ø³Ù… Ø­Ø¬Ø² Ø·Ø§ÙˆÙ„Ø§Øª ÙˆØªÙ‚ÙŠÙŠÙ…Ø§Øª",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "portfolio": [
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… Ù…ÙŠÙ†ÙŠÙ…Ø§Ù„ ÙŠØ¹Ø±Ø¶ Ø£Ø¹Ù…Ø§Ù„ÙŠ Ø¨Ø§Ø­ØªØ±Ø§Ù",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø®Ø¨Ø±Ø§Øª",
            "Ø£Ø¨ÙŠ Ù‚Ø³Ù… ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø±Ø³Ø§Ø¦Ù„",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "blog": [
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… Ù‚Ø±Ø§Ø¡Ø© Ù…Ø±ÙŠØ­ Ù…Ø¹ Ø®Ø·ÙˆØ· ÙƒØ¨ÙŠØ±Ø©",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ø¢Ø®Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø¹ ØªØµÙ†ÙŠÙØ§Øª",
            "Ø£Ø¨ÙŠ sidebar Ù…Ø¹ Ø¨Ø­Ø« ÙˆÙ…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ù‡ÙˆØ±Ø©",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "realestate": [
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… ÙØ§Ø®Ø± ÙŠØ¹Ø±Ø¶ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø¨Ø§Ø­ØªØ±Ø§Ù",
            "Ø£Ø¶Ù ÙÙ„ØªØ± Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© ÙˆØ§Ù„Ø³Ø¹Ø± ÙˆØ§Ù„Ù…Ø³Ø§Ø­Ø©",
            "Ø£Ø¨ÙŠ Ù‚Ø³Ù… Ø¹Ù‚Ø§Ø±Ø§Øª Ù…Ù…ÙŠØ²Ø© Ù…Ø¹ ØµÙˆØ± ÙƒØ¨ÙŠØ±Ø©",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
        "general": [
            "ÙˆØ´ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø£Ùˆ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù„ÙŠ ØªØ¨ÙŠ ØªØ¹Ø±Ø¶Ù‡Ø§ØŸ",
            "Ø£Ø¨ÙŠ ØªØµÙ…ÙŠÙ… ÙØ§Ø®Ø± ÙˆØ£Ù†ÙŠÙ‚ Ø¨Ø£Ù„ÙˆØ§Ù† Ø¯Ø§ÙƒÙ†Ø©",
            "Ø£Ø¶Ù Ù‚Ø³Ù… Ø¹Ø±ÙˆØ¶ ÙˆØªØ®ÙÙŠØ¶Ø§Øª Ù…Ø¹ Ø¹Ø¯Ø§Ø¯",
            "Ù†ÙÙ‘Ø° Ø§Ù„ØªØµÙ…ÙŠÙ… ğŸš€",
        ],
    }
    base = suggestions_map.get(store_type, suggestions_map["general"])
    msg_lower = message.lower()
    return [s for s in base if not any(word in msg_lower for word in s.split()[:2])][:4]


@router.post("/conversation", response_model=AIConversationResponse)
@limiter.limit("20/minute")
async def ai_conversation(
    request: Request,
    body: AIConversationRequest,
    current_user: CurrentUser,
    db: Annotated[AsyncSession, Depends(get_db)],
):
    """Conversational AI endpoint â€” chat about the store without generating HTML."""
    start_time = time.time()

    system_prompt = CONVERSATION_SYSTEM_PROMPT.format(
        store_name=body.store_name,
        store_type=body.store_type,
    )

    # Build messages list (keep last 20 messages for context)
    messages = []
    for msg in body.conversation_history[-20:]:
        role = msg.get("role", "user")
        if role == "ai":
            role = "assistant"
        if role in ("user", "assistant"):
            messages.append({"role": role, "content": msg.get("content", "")})
    messages.append({"role": "user", "content": body.message})

    reply = ""
    anthropic_key = settings.ANTHROPIC_API_KEY
    openai_key = settings.OPENAI_API_KEY
    google_key = settings.GOOGLE_API_KEY

    # Try providers in order
    if anthropic_key:
        try:
            reply = await _call_anthropic_conversation(messages, system_prompt, anthropic_key)
        except Exception as e:
            logger.warning(f"Anthropic conversation error: {e}")

    if not reply and openai_key:
        try:
            reply = await _call_openai_conversation(messages, system_prompt, openai_key)
        except Exception as e:
            logger.warning(f"OpenAI conversation error: {e}")

    if not reply and google_key:
        try:
            reply = await _call_gemini_conversation(messages, system_prompt, google_key)
        except Exception as e:
            logger.warning(f"Gemini conversation error: {e}")

    if not reply:
        reply = (
            f"Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸ‘‹ Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ù„Ø¨Ù†Ø§Ø¡ Ù…ØªØ¬Ø± \"{body.store_name}\".\n\n"
            "Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù† Ø±Ø¤ÙŠØªÙƒ Ù„Ù„Ù…ØªØ¬Ø± â€” Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø§Ù„Ø³ØªØ§ÙŠÙ„ØŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª â€” "
            "ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ù‚ÙˆÙ„ \"Ù†ÙÙ‘Ø°\" ÙˆØ£Ù†Ø§ Ø£Ø¨Ù†ÙŠÙ‡ Ù„Ùƒ! ğŸš€"
        )

    execution_time = round(time.time() - start_time, 2)

    # Save to Supabase
    try:
        await _save_conversation_to_supabase(
            user_id=str(current_user.id),
            store_id=None,
            message=body.message,
            response=reply,
            execution_time=execution_time,
        )
    except Exception:
        pass

    return AIConversationResponse(
        reply=reply,
        suggestions=_get_conversation_suggestions(body.message, body.store_type),
        should_execute=False,
        execution_time=execution_time,
    )


@router.post("/chat", response_model=AIChatResponse)
@limiter.limit("10/minute")
async def ai_chat(
    request: Request,
    body: AIChatRequest,
    current_user: CurrentUser,
    db: Annotated[AsyncSession, Depends(get_db)],
):
    """Process an AI chat message and return updated store HTML."""
    start_time = time.time()
    
    anthropic_key = settings.ANTHROPIC_API_KEY
    openai_key = settings.OPENAI_API_KEY
    google_key = settings.GOOGLE_API_KEY
    new_html = ""
    response_message = ""
    provider_used = "local"

    # Priority 1: Anthropic Claude (primary)
    if anthropic_key and not new_html:
        try:
            new_html = await _call_anthropic_chat(
                body.current_html, body.message, anthropic_key,
            )
            response_message = f"âœ… Claude: ØªÙ… ØªØ·Ø¨ÙŠÙ‚ '{body.message}' Ø¨Ø°ÙƒØ§Ø¡"
            provider_used = "anthropic"
        except Exception as e:
            logger.warning(f"Anthropic error: {e}")

    # Priority 2: OpenAI GPT (first fallback)
    if not new_html and openai_key:
        try:
            new_html = await _call_openai_chat(
                body.current_html, body.message, openai_key,
            )
            response_message = f"âœ… GPT: ØªÙ… ØªØ·Ø¨ÙŠÙ‚ '{body.message}'"
            provider_used = "openai"
        except Exception as e:
            logger.warning(f"OpenAI error: {e}")

    # Priority 3: Google Gemini (second fallback)
    if not new_html and google_key:
        try:
            new_html = await _call_gemini_chat(
                body.current_html, body.message, google_key,
            )
            response_message = f"âœ… Gemini: ØªÙ… ØªØ·Ø¨ÙŠÙ‚ '{body.message}'"
            provider_used = "google"
        except Exception as e:
            logger.warning(f"Gemini error: {e}")

    # Priority 4: Local modifications (offline fallback)
    if not new_html:
        new_html, description = _apply_local_modifications(
            body.current_html, body.message,
        )
        response_message = f"{description} âœ…"
        provider_used = "local"
    
    execution_time = round(time.time() - start_time, 2)
    
    # Save conversation to Supabase (async, non-blocking)
    try:
        await _save_conversation_to_supabase(
            user_id=str(current_user.id),
            store_id=body.store_id,
            message=body.message,
            response=response_message,
            html_before=body.current_html[:500] if body.current_html else None,
            html_after=new_html[:500] if new_html else None,
            execution_time=execution_time,
        )
    except Exception as e:
        logger.debug(f"Supabase save skipped: {e}")
    
    return AIChatResponse(
        html=new_html,
        message=response_message,
        suggestions=_get_suggestions(body.message),
        execution_time=execution_time,
    )


def _get_suggestions(last_message: str) -> list[str]:
    """Return context-aware suggestions based on the last message."""
    suggestions = [
        "Ø­ÙˆÙ‘Ù„ Ø§Ù„ØªØµÙ…ÙŠÙ… Ù„ÙˆØ¶Ø¹ Ø¯Ø§ÙƒÙ† ÙØ§Ø®Ø±",
        "Ø£Ø¶Ù Ù‚Ø³Ù… Ø¹Ø±ÙˆØ¶ Ù…Ø¹ Ø¹Ø¯Ø§Ø¯ ØªÙ†Ø§Ø²Ù„ÙŠ",
        "Ø£Ø¶Ù Ø­Ø±ÙƒØ§Øª scroll animations Ù„ÙƒÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…",
        "Ø­Ø³Ù‘Ù† Ø§Ù„Ù€ Hero Ø¨ØªØ¯Ø±Ø¬ Ù„ÙˆÙ†ÙŠ ÙˆØ²Ø± CTA Ø£ÙƒØ¨Ø±",
        "Ø£Ø¶Ù Ù‚Ø³Ù… ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø¹ Ù†Ø¬ÙˆÙ…",
        "Ø£Ø¶Ù Ù‚Ø³Ù… Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© FAQ",
        "ØºÙŠÙ‘Ø± Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø®Ø¶Ø±Ø§Ø¡ ÙˆØ°Ù‡Ø¨ÙŠØ©",
    ]
    msg_lower = last_message.lower()
    return [s for s in suggestions if not any(word in msg_lower for word in s.split()[:2])][:4]


# â•â•â•â•â•â•â• Ø§Ø®ØªØ¨Ø§Ø± AI (Ù…Ø­Ù…ÙŠ â€” dev ÙÙ‚Ø·) â•â•â•â•â•â•â•
@router.post("/test", response_model=AIChatResponse)
async def ai_chat_test_endpoint(
    request: Request,
    req: AIChatRequest,
) -> AIChatResponse:
    """Ø§Ø®ØªØ¨Ø§Ø± AI Chat â€” ÙŠØ¹Ù…Ù„ ÙÙ‚Ø· ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±."""
    if settings.is_production:
        raise HTTPException(status_code=404, detail="Not found")
    start_time = time.time()
    
    api_key = settings.ANTHROPIC_API_KEY
    
    if not api_key:
        # Try OpenAI
        if settings.OPENAI_API_KEY:
            try:
                html = await _call_openai_chat(req.current_html, req.message, settings.OPENAI_API_KEY)
                return AIChatResponse(
                    html=html,
                    message="âœ… GPT: ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªØ¬Ø±",
                    execution_time=round(time.time() - start_time, 2),
                )
            except Exception:
                pass
        
        # Try Gemini
        if settings.GOOGLE_API_KEY:
            try:
                html = await _call_gemini_chat(req.current_html, req.message, settings.GOOGLE_API_KEY)
                return AIChatResponse(
                    html=html,
                    message="âœ… Gemini: ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªØ¬Ø±",
                    execution_time=round(time.time() - start_time, 2),
                )
            except Exception:
                pass
        
        # Local fallback
        modified_html, message = _apply_local_modifications(req.current_html, req.message)
        return AIChatResponse(
            html=modified_html,
            message=f"âœ… ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ù…Ø­Ù„ÙŠØ©: {message}",
            execution_time=round(time.time() - start_time, 2),
        )
    
    try:
        enhanced_html = await _call_anthropic_chat(
            current_html=req.current_html,
            user_message=req.message,
            api_key=api_key,
        )
        
        return AIChatResponse(
            html=enhanced_html,
            message="ğŸ¨ Claude Ø¹Ø¯Ù‘Ù„ Ù…ØªØ¬Ø±Ùƒ Ø¨Ø°ÙƒØ§Ø¡! Ø´ÙˆÙ Ø§Ù„Ù†ØªÙŠØ¬Ø© ğŸ‘ˆ",
            execution_time=round(time.time() - start_time, 2),
        )
    except Exception as e:
        logger.error(f"Claude API error in test: {type(e).__name__}: {e}")
        modified_html, message = _apply_local_modifications(req.current_html, req.message)
        return AIChatResponse(
            html=modified_html,
            message=f"âš ï¸ Ù†Ø¸Ø§Ù… Ù…Ø­Ù„ÙŠ: {message}",
            execution_time=round(time.time() - start_time, 2),
        )
